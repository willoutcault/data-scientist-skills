---
title: "Web Scrape & Database Ingestion - Project 3"
author: "Paul Perez, William Outcault, Aaron Zalki, John Suh"
date: "10/20/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages(c("sqldf", "rvest", "dplyr", "tidyr", "stringr"))
```
```{r}
library(sqldf)
library(rvest)
library(dplyr)
library(tidyr)
library(stringr)
```

### Connect to Database
```{r}
db <- dbConnect(SQLite(), dbname = "project_3.sqlite")
```

### SelectorGadget Web Scraping & Dataframe Functions
Creating functions for pulling each element of the web scrape using [SelectorGadget](https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html) and Rvest.
```{r}
# Scrape location of job
get_location <- function(x){
     location <- x %>% read_html() %>% html_nodes(".location") %>% html_text()
     location <- sub("\\s+$", "", location)
     return(location)
}

# Scrape desired skills for job
get_skills <- function(x) {
     skills <- x %>% read_html() %>% html_nodes(".skill-list") %>% html_text()
     skills <- str_extract_all (skills,"\\b[A-z].+\\b")
     return(skills)
}   

# Scrape job title
get_titles <- function(x) {
     titles <- x %>% read_html() %>% html_nodes(".job-title") %>% html_text()
     titles <- gsub("^\\s+|\\s+$", "", titles)
     return(titles)
}

# Scrape salary
get_wage <- function(x) {
     wage <- x %>% read_html() %>% html_nodes(".wage") %>% html_text()
     return(wage)
}     

# Scrape date of posting
get_date <- function(x) {
     date <- x %>% read_html() %>% html_nodes(".posted") %>% html_text()
     date <- str_replace_all(date, "Posted", "")
     return(date)
}
```

Create a function to take in a URL, and variable (which declares the job query), use that URL to  scrape pages for all relevent elements of which will be put into a data frame.
```{r}
create_df <- function(x, y) {
  location <- get_location(x)
  skills <- get_skills(x)
  titles <- get_titles(x)
  wage <- get_wage(x)
  date <- get_date(x)
  df <- data.frame(cbind(titles, date, location, wage, skills))
  df <- unnest(df, titles)
  df <- unnest(df, date)
  df <- unnest(df, location)
  df <- unnest(df, wage)
  df <- unnest(df, skills)
  df$query <- y
  levels(df$wage)[levels(df$wage) == "Compensation Unspecified"]<- "NULL"
  return(df)
}
```

## Cyber Coders
### Data Scientist Scrape

We'll loop through the number of pages of job posting available on Cyber Coders for Data Scientist roles.
```{r}
ds <- "Data Science"
ds_urls <- list()

for (i in 1:4) {
    ds_urls[i] <- paste("https://www.cybercoders.com/search/?page=", i, "&searchterms=Data%20Scientist&searchlocation=&newsearch=true&originalsearch=true&sorttype=", sep = "")
}

ds_df1 <- create_df(ds_urls[[1]], ds)
ds_df2 <- create_df(ds_urls[[2]], ds)
ds_df3 <- create_df(ds_urls[[3]], ds)
ds_df4 <- create_df(ds_urls[[4]], ds)
```

Next we'll ingest these data frames into our 'cybercoders' table in our database.
```{r}
dbWriteTable(conn = db, name="cybercoders", value=ds_df1, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=ds_df2, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=ds_df3, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=ds_df4, row.names = FALSE, header = TRUE, append = TRUE)
```

### Data Engineer Scrape
```{r}
de <- "Data Engineer"
de_urls <- list()

for (i in 1:10) {
  de_urls[i] <- paste("https://www.cybercoders.com/search/?page=", i, "&searchterms=Data%20Engineer&searchlocation=&newsearch=true&originalsearch=true&sorttype=", sep = "")
}

de_df1 <- create_df(de_urls[[1]], de)
de_df2 <- create_df(de_urls[[2]], de)
de_df3 <- create_df(de_urls[[3]], de)
de_df4 <- create_df(de_urls[[4]], de)
de_df5 <- create_df(de_urls[[5]], de)
de_df6 <- create_df(de_urls[[6]], de)
de_df7 <- create_df(de_urls[[7]], de)
de_df8 <- create_df(de_urls[[8]], de)
de_df9 <- create_df(de_urls[[9]], de)
de_df10 <- create_df(de_urls[[10]], de)
```

```{r}
dbWriteTable(conn = db, name="cybercoders", value=de_df1, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=de_df2, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=de_df3, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=de_df4, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=de_df5, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=de_df6, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=de_df7, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=de_df8, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=de_df9, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=de_df10, row.names = FALSE, header = TRUE, append = TRUE)
```

### Data Analyst Scrape

```{r}
da <- "Data Analyst"
da_urls <- list()

for (i in 1:3) {
  da_urls[i] <- paste("https://www.cybercoders.com/search/?page=", i, "&searchterms=Data%20Analyst&searchlocation=&newsearch=true&originalsearch=true&sorttype=", sep = "")
}

da_df1 <- create_df(da_urls[[1]], da)
da_df2 <- create_df(da_urls[[2]], da)
da_df3 <- create_df(da_urls[[3]], da)
```

```{r}
dbWriteTable(conn = db, name="cybercoders", value=da_df1, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=da_df2, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="cybercoders", value=da_df3, row.names = FALSE, header = TRUE, append = TRUE)
```


## Career Builder 

A slightly different approach utilizing the chrome extension [Web Scraper](https://chrome.google.com/webstore/detail/web-scraper/jnhgnonknehpejjnehehllkliplmbmhn?hl=en) to actually scrape the pages and produce an output .csv file.
```{r}
cb_ds <- data.frame(read.csv(file = "datascientist.csv"))
names(cb_ds) <- c("ws_order", "ws_st_url", "link", "link_href", "load_more", "load_more_href", "name", "location", "salary", "skills")
cb_ds$query <- "Data Science"

cb_de <- data.frame(read.csv(file = "dataengineer.csv"))
names(cb_de) <- c("ws_order", "ws_st_url", "link", "link_href", "load_more", "load_more_href", "name", "location", "salary", "skills")
cb_de$query <- "Data Engineer"
```

We noticed that the 'Data Scientist' and 'Data Engineer' queries produced the same formatted .csv compared to the 'Data Analyst' query. This only added one additional step to reorder the 'Data Analayst' data frame.
```{r}
cb_da <- data.frame(read.csv(file = "dataanalyst.csv"))
names(cb_da) <- c("ws_order", "ws_st_url", "link", "link_href", "salary", "location", "skills", "load_more", "load_more_href", "name")
cb_da <- cb_da[c("ws_order", "ws_st_url", "link", "link_href", "load_more", "load_more_href", "name", "location", "salary", "skills")]
cb_da$query <- "Data Analyst"
```

```{r}
dbWriteTable(conn = db, name="careerbuilder", value=cb_ds, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="careerbuilder", value=cb_de, row.names = FALSE, header = TRUE, append = TRUE)
dbWriteTable(conn = db, name="careerbuilder", value=cb_da, row.names = FALSE, header = TRUE, append = TRUE)
```

### Retrieve Data from the Database
```{r}
blended_view <- data.frame(dbReadTable(db, "job_postings"))
cyber_coders <- data.frame(dbReadTable(db, "cybercoders"))
career_builder <- data.frame(dbReadTable(db, "careerbuilder"))

#View(blended_view)
#View(cyber_coders)
#View(career_builder)
```

### Extract to .csv files
```{r}
write.csv(blended_view, "blended_view.csv")
write.csv(cyber_coders, "cyber_coders.csv")
write.csv(career_builder, "career_builder.csv")
```

### Close Connection
```{r}
dbDisconnect(db)
```